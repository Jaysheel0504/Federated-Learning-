{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Machine Learning Assignment 2 - Federated Learning**\n",
        "\n",
        "submitted by :-\n",
        "\n",
        "*   Praneet Karna\n",
        "*   Jaysheel Shah\n",
        "*   Nishal Shah\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EXW66ytLt3mP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training using Federated Learning (10 clients)\n",
        "\n"
      ],
      "metadata": {
        "id": "5IFc84lhuHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive to get the data"
      ],
      "metadata": {
        "id": "3k4nVJZ5uSp7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXxXnWuf-LwZ",
        "outputId": "c230835e-3226-456c-d4a2-eec4fb189619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required libraries - Numpy and Pandas"
      ],
      "metadata": {
        "id": "clEL3mvHuWJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "from torchvision import datasets\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "_KCtPI6HL4ia"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating custom dataset class suited to our folder structure"
      ],
      "metadata": {
        "id": "3Ijwmt6AubLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, directory, transform_function=None, target_size=(256,256)):\n",
        "        self.directory = directory\n",
        "        self.transform_function = transform_function\n",
        "        self.target_size = target_size\n",
        "\n",
        "        self.labels = [label for label in sorted(os.listdir(directory))\n",
        "                             if os.path.isdir(os.path.join(directory, label))]\n",
        "        self.label_to_index = {label: index for index, label in enumerate(self.labels)}\n",
        "\n",
        "        # Build a list of images and corresponding labels\n",
        "        self.samples = []\n",
        "        for label in self.labels:\n",
        "            path = os.path.join(directory, label)\n",
        "            if os.path.isdir(path):\n",
        "              for filename in os.listdir(path):\n",
        "                filepath = os.path.join(path, filename)\n",
        "                if os.path.isfile(filepath):\n",
        "                  self.samples.append((filepath, self.label_to_index[label]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      path, label = self.samples[index]\n",
        "      try:\n",
        "          image = Image.open(path).convert(\"RGB\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error opening image at {path}: {e}\")\n",
        "          raise\n",
        "\n",
        "      if self.transform_function:\n",
        "          image = self.transform_function(image)\n",
        "\n",
        "      return image, label\n"
      ],
      "metadata": {
        "id": "MN_jnbbQL9dx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a custom CNN network for animal image classification with 4 classes"
      ],
      "metadata": {
        "id": "pgbbSDPTufnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(32 * 64 * 64, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        # print(\"After conv1:\", x.shape)\n",
        "        x = self.relu1(x)\n",
        "        # print(\"After relu1:\", x.shape)\n",
        "        x = self.pool1(x)\n",
        "        # print(\"After pool1:\", x.shape)\n",
        "        x = self.conv2(x)\n",
        "        # print(\"After conv2:\", x.shape)\n",
        "        x = self.relu2(x)\n",
        "        # print(\"After relu2:\", x.shape)\n",
        "        x = self.pool2(x)\n",
        "        # print(\"After pool2:\", x.shape)\n",
        "        x = self.flatten(x)\n",
        "        # print(\"After flatten:\", x.shape)\n",
        "        x = self.fc1(x)\n",
        "        # print(\"After fc1:\", x.shape)\n",
        "        x = self.relu3(x)\n",
        "        # print(\"After relu3:\", x.shape)\n",
        "        x = self.fc2(x)\n",
        "        # print(\"After fc2:\", x.shape)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KCvQdhPhOapV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to filter clients during each epoch based on metadata (battery life and signal strength).\n",
        "\n",
        "Current requirements : 30% battery and above, 2/5 signal strength and above."
      ],
      "metadata": {
        "id": "2dq0Ss2gumBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def approve_client(client_number, epoch_number):\n",
        "  meta_data_path = '/content/drive/My Drive/DividedLabeledDataset/metadata.csv'\n",
        "  meta_data = pd.read_csv(meta_data_path)\n",
        "  index = epoch_number*10 + client_number - 1\n",
        "\n",
        "  if(meta_data.at[index, 'battery life']>=30 and meta_data.at[index,'signal strength']>=2):\n",
        "    return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "8y7v5W3wPf4q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get average of weights from a list of models"
      ],
      "metadata": {
        "id": "0mgsk57Fu631"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_average_of_models(models):\n",
        "    n = len(models)\n",
        "    average = models[0].state_dict()\n",
        "\n",
        "    for i in range(1, n):\n",
        "        current = models[i].state_dict()\n",
        "        average = {x: average[x] + current[x] for x in average}\n",
        "\n",
        "    average = {x: parameter / n for x, parameter in average.items()}\n",
        "\n",
        "    return average"
      ],
      "metadata": {
        "id": "VRMp_XlaRzmv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the folder names containing data for every epoch for every client.\n",
        "\n",
        "Currently : 10 clients and 4 total epochs"
      ],
      "metadata": {
        "id": "oRoJ1iqRvAa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folderNames_epoch1 =  ['Part_1', 'Part_2', 'Part_3', 'Part_4', 'Part_5', 'Part_6', 'Part_7', 'Part_8', 'Part_9', 'Part_10']\n",
        "folderNames_epoch2 =  ['Part_11', 'Part_12', 'Part_13', 'Part_14', 'Part_15', 'Part_16', 'Part_17', 'Part_18', 'Part_19', 'Part_20']\n",
        "folderNames_epoch3 =  ['Part_21', 'Part_22', 'Part_23', 'Part_24', 'Part_25', 'Part_26', 'Part_27', 'Part_28', 'Part_29', 'Part_30']\n",
        "folderNames_epoch4 =  ['Part_31', 'Part_32', 'Part_33', 'Part_34', 'Part_35', 'Part_36', 'Part_37', 'Part_38', 'Part_39', 'Part_40']\n",
        "\n",
        "folderNames_list = [folderNames_epoch1,folderNames_epoch2,folderNames_epoch3,folderNames_epoch4]"
      ],
      "metadata": {
        "id": "HBODmV56RhsF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function where the federated learning is performed for 4 epochs"
      ],
      "metadata": {
        "id": "ZD9eirDTvK1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_federated_learning(base_model, num_epochs, learning_rate, list_of_client_lists):\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      folder_name_list = []\n",
        "      client_list = folderNames_list[epoch]\n",
        "\n",
        "      # selecting clients\n",
        "      for client in range(10):\n",
        "        if (approve_client(client+1,epoch)):\n",
        "          folder_name_list.append(\"/content/drive/My Drive/DividedLabeledDataset/\" + folderNames_list[epoch][client])\n",
        "\n",
        "      trained_client_models = []\n",
        "\n",
        "      for folder in folder_name_list:\n",
        "            print(f\"Training started for {folder}\");\n",
        "            current_client_model = CustomCNN()\n",
        "            current_client_model.load_state_dict(base_model.state_dict())\n",
        "            current_client_model.train\n",
        "\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.SGD(current_client_model.parameters(), lr=learning_rate)\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])\n",
        "\n",
        "            training_data = Dataset(folder, transform_function=transform)\n",
        "            training_data_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "\n",
        "            for inputs, labels in training_data_loader:\n",
        "                outputs = current_client_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            trained_client_models.append(current_client_model)\n",
        "\n",
        "      print(f\"Training completed for epoch {epoch}\")\n",
        "\n",
        "      # averaging the weights of trained client models\n",
        "      base_model.load_state_dict(get_average_of_models(trained_client_models))\n",
        "      base_model.eval()\n",
        "\n",
        "      transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])\n",
        "\n",
        "      testing_data = Dataset(\"/content/drive/MyDrive/DividedLabeledDataset/test\", transform_function=transform)\n",
        "      testing_data_loader = DataLoader(testing_data, batch_size=32, shuffle=True)\n",
        "\n",
        "      num_total_examples = 0\n",
        "      num_correct_examples = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for inp, lbs in testing_data_loader:\n",
        "          outputs = base_model(inp)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          num_correct_examples += (predicted == lbs).sum().item()\n",
        "          num_total_examples += lbs.size(0)\n",
        "\n",
        "      testing_accuracy = num_correct_examples / num_total_examples\n",
        "      print(f'Testing Accuracy for epoch {epoch}: {100 * testing_accuracy:.2f}%')\n",
        "\n",
        "      # Print Confusion Matrix\n",
        "      print(\"Confusion Matrix:\")\n",
        "      with torch.no_grad():\n",
        "          all_labels = []\n",
        "          all_predictions = []\n",
        "          for inputs, labels in testing_data_loader:\n",
        "              outputs = base_model(inputs)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "              all_labels.extend(labels.numpy())\n",
        "              all_predictions.extend(predicted.numpy())\n",
        "\n",
        "          cm = confusion_matrix(all_labels, all_predictions)\n",
        "          print(cm)\n",
        "\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs} completed\")"
      ],
      "metadata": {
        "id": "945CKJzgSBLO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the federated learning function on a new base model"
      ],
      "metadata": {
        "id": "UfbQjt9Sw13r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = CustomCNN()\n",
        "perform_federated_learning(base_model, num_epochs=4, learning_rate=0.001, list_of_client_lists=folderNames_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAvmqZHrS-6F",
        "outputId": "bba630ea-c943-40d7-cb8e-5ae55949fb61"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_1\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_3\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_5\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_6\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_8\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_9\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_10\n",
            "Training completed for epoch 0\n",
            "Testing Accuracy for epoch 0: 20.96%\n",
            "Confusion Matrix:\n",
            "[[ 65 135   0   0]\n",
            " [ 31  84   0   0]\n",
            " [ 46 187   0   0]\n",
            " [ 35 128   0   0]]\n",
            "Epoch 1/4 completed\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_11\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_13\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_14\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_16\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_17\n",
            "Training completed for epoch 1\n",
            "Testing Accuracy for epoch 1: 22.22%\n",
            "Confusion Matrix:\n",
            "[[ 72 120   8   0]\n",
            " [ 34  76   5   0]\n",
            " [ 51 172  10   0]\n",
            " [ 41 120   2   0]]\n",
            "Epoch 2/4 completed\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_22\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_23\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_24\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_30\n",
            "Training completed for epoch 2\n",
            "Testing Accuracy for epoch 2: 17.30%\n",
            "Confusion Matrix:\n",
            "[[  0 190  10   0]\n",
            " [  0 110   5   0]\n",
            " [  0 220  13   0]\n",
            " [  0 158   5   0]]\n",
            "Epoch 3/4 completed\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_33\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_38\n",
            "Training started for /content/drive/My Drive/DividedLabeledDataset/Part_40\n",
            "Training completed for epoch 3\n",
            "Testing Accuracy for epoch 3: 28.97%\n",
            "Confusion Matrix:\n",
            "[[ 57  50  93   0]\n",
            " [ 29  44  42   0]\n",
            " [ 33  95 105   0]\n",
            " [ 32  52  79   0]]\n",
            "Epoch 4/4 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training without using Federated Learning"
      ],
      "metadata": {
        "id": "LGb2XqB8twtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model on the same training data"
      ],
      "metadata": {
        "id": "PSKrPQ1Y2h4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomCNN()\n",
        "model.train\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "          ])\n",
        "\n",
        "training_dataset = Dataset(\"/content/drive/MyDrive/imge-classification-animal/animal-classification/train\", transform_function=transform)\n",
        "training_dataset_loader = DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(f\"Training started.\")\n",
        "\n",
        "for inp, lbs in training_dataset_loader:\n",
        "  outputs = model(inp)\n",
        "  loss = criterion(outputs, lbs)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "print(f\"Training ended.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTfh_lswTCo2",
        "outputId": "5cd705c4-40bf-4712-fad6-ab1aa3d1f155"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started.\n",
            "Training ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking accuracy of model trained without federated learning"
      ],
      "metadata": {
        "id": "xLGcQ5xFwHJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data = Dataset(\"/content/drive/MyDrive/DividedLabeledDataset/test\", transform_function=transform)\n",
        "testing_data_loader = DataLoader(testing_data, batch_size=32, shuffle=True)\n",
        "\n",
        "num_total_examples = 0\n",
        "num_correct_examples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for inp, lbs in testing_data_loader:\n",
        "    outputs = model(inp)\n",
        "    x, predicted = torch.max(outputs.data, 1)\n",
        "    num_correct_examples += (predicted == lbs).sum().item()\n",
        "    num_total_examples += lbs.size(0)\n",
        "\n",
        "testing_accuracy = num_correct_examples / num_total_examples\n",
        "# printing the training accuracy of this epoch of federated learning\n",
        "print(f'Testing Accuracy without federated learning: {100 * testing_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuSw4oXXwGjM",
        "outputId": "6a7bdebb-0b65-4736-c7e2-cfe35e71c5fa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy without federated learning: 32.77%\n"
          ]
        }
      ]
    }
  ]
}